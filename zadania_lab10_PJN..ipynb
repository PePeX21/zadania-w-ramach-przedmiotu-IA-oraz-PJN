{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcjJ-YZy5-Pl"
   },
   "source": [
    "# Wykrywanie encji nazwanych z Flair\n",
    "\n",
    "To już ostatnie laboratoria zadaniowe, w związku z tym, jeśli znajdziecie chwilę wolnego czasu, wypełnijcie proszę ankietę: https://docs.google.com/forms/d/1rHPjpL70XdXRD-ILl3AHophPNUk0AhsFus1-mtkUPsI\n",
    "\n",
    "Pozwoli to mi poprawić laboratoria w przyszłości, z góry dziękuję :)\n",
    "\n",
    "# Flair\n",
    "\n",
    "Biblioteka Flair to bardzo popularne narzędzie do tagowania sekwencji. Zaintstalujmy ją"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4inJhzI0wQmM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
      "Collecting gdown==4.4.0\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (0.7.0)\n",
      "Collecting conllu>=4.0\n",
      "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (2022.3.15)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-8.13.0-py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: tabulate in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (0.8.9)\n",
      "Requirement already satisfied: gensim>=3.4.0 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (4.1.2)\n",
      "Collecting sentencepiece==0.1.95\n",
      "  Downloading sentencepiece-0.1.95-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: transformers>=4.0.0 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (4.19.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sphinx 4.4.0 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia-api\n",
      "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (4.64.0)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (3.5.1)\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (1.0.2)\n",
      "Collecting pptree\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "Collecting janome\n",
      "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "Requirement already satisfied: lxml in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (4.8.0)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (2.8.2)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from flair) (1.10.2)\n",
      "Collecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\asia\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (2.27.1)\n",
      "Requirement already satisfied: six in c:\\users\\asia\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asia\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (3.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\asia\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.21.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (1.7.3)\n",
      "Requirement already satisfied: future in c:\\users\\asia\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.18.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\asia\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.0.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.7.1)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.26.9)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (2.2.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\asia\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asia\\anaconda3\\lib\\site-packages (from tqdm>=4.26.0->flair) (0.4.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\asia\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
      "Building wheels for collected packages: gdown, mpld3, overrides, sqlitedict, langdetect, pptree, wikipedia-api\n",
      "  Building wheel for gdown (PEP 517): started\n",
      "  Building wheel for gdown (PEP 517): finished with status 'done'\n",
      "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14759 sha256=f53d2f093b03092506dad11dfeaa9ba81cf09bd424f98865647be45cea038d68\n",
      "  Stored in directory: c:\\users\\asia\\appdata\\local\\pip\\cache\\wheels\\7d\\37\\b6\\b2a79c75e898c0b8e46ff255102602d7159a10d9af0d80641a\n",
      "  Building wheel for mpld3 (setup.py): started\n",
      "  Building wheel for mpld3 (setup.py): finished with status 'done'\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=383053503e3659f55c80ecdab225c69881746466815ef9193c47e8bb9b09aaee\n",
      "  Stored in directory: c:\\users\\asia\\appdata\\local\\pip\\cache\\wheels\\a6\\f4\\e6\\e40ff9021f6b3854af70fa8ea004f5ab95672817462df08fed\n",
      "  Building wheel for overrides (setup.py): started\n",
      "  Building wheel for overrides (setup.py): finished with status 'done'\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=54eb3007ddf832c934d7e0dff96b6f43b92605233cadb9429aeedf4eda44315d\n",
      "  Stored in directory: c:\\users\\asia\\appdata\\local\\pip\\cache\\wheels\\7d\\11\\0e\\73fdcb3d71d97e33c230900efe85923ee9d49515d050503174\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15733 sha256=85b30aadafc6422b25f4479298d082e7ce4f20044e0b18805401b8451b6c9d6c\n",
      "  Stored in directory: c:\\users\\asia\\appdata\\local\\pip\\cache\\wheels\\48\\a5\\80\\fa89dc26af0f4c280b500f5529978552379c1ce8907e0a281c\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=0375be074e7de92ea909102b46e347c2bd06c4b977aed69d11a9cffa3fa54057\n",
      "  Stored in directory: c:\\users\\asia\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "  Building wheel for pptree (setup.py): started\n",
      "  Building wheel for pptree (setup.py): finished with status 'done'\n",
      "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4630 sha256=722ac8e26d788ae3d0c4b47fd6674ee0cd2d7774882048b38d69220bcdf4e32b\n",
      "  Stored in directory: c:\\users\\asia\\appdata\\local\\pip\\cache\\wheels\\52\\0e\\51\\514e690004ea9713bc3fdb678d5e2768fcc597d0c3b6a3abd2\n",
      "  Building wheel for wikipedia-api (setup.py): started\n",
      "  Building wheel for wikipedia-api (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13476 sha256=c590ba9f29b99ccb0c6ae26dc02fe7b89c32a2bfcb343806886379bf092220b5\n",
      "  Stored in directory: c:\\users\\asia\\appdata\\local\\pip\\cache\\wheels\\c7\\cf\\1a\\c300428dd51654cdadc921abdff75acaa7cc80b7151a2f0695\n",
      "Successfully built gdown mpld3 overrides sqlitedict langdetect pptree wikipedia-api\n",
      "Installing collected packages: sentencepiece, py4j, overrides, importlib-metadata, wikipedia-api, sqlitedict, segtok, pptree, mpld3, more-itertools, langdetect, konoha, janome, hyperopt, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "Successfully installed bpemb-0.3.3 conllu-4.4.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 gdown-4.4.0 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.13.0 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.5 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 wikipedia-api-0.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-maEaJD0Ojj"
   },
   "outputs": [],
   "source": [
    "# OPCJONALNE WSPARCIE DLA KART GRAFICZNYCH\n",
    "# W colabie możemy trenować z wykorzystaniem karty graficznej, dzięki temu trening działa dużo szybciej\n",
    "# Aby włączyć wsparcie dla karty graficznej musimy:\n",
    "# 1. w menu 'srodowisko wykonawcze' wybrać `zmien typ srodowiska wykonawczego` i tam `akcelerator sprzętowy` = GPU\n",
    "# 2. odkomentować linijki poniżej\n",
    "# import flair, torch\n",
    "# flair.device = torch.device('cuda:0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyApRy6G7YQk"
   },
   "source": [
    "# Ładowanie zbioru danych i słownika z etykietami.\n",
    "\n",
    "**Zadanie1: (1 punkt):** Stwórz słownik etykiet z wczytanego korpusu korzystając z funkcji `make_label_dictionary()`. W naszym zbiorze, etykiety do wykrycia występują w kolumnie `ner`, której identyfikator został zapisany w linijce 6. Tutorial: https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md może okazać się pomocny.\n",
    "\n",
    "Efektem działania powinna być lista etykiet np: \n",
    "`Dictionary with 20 tags: <unk>, Variable, Class, Application, User_Interface_Element, Code_Block, Language, Function, Data_Structure, Library, Data_Type, File_Type, File_Name, Version, HTML_XML_Tag, Device, Operating_System, Website, User_Name, Algorithm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eK84adKF6GLr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:23:54,966 https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/train.txt not found in cache, downloading to C:\\Users\\Asia\\AppData\\Local\\Temp\\tmplibrfded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3221198B [00:00, 24593139.13B/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:23:55,224 copying C:\\Users\\Asia\\AppData\\Local\\Temp\\tmplibrfded to cache at C:\\Users\\Asia\\.flair\\datasets\\ner_english_stackoverflow\\train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:23:55,441 removing temp file C:\\Users\\Asia\\AppData\\Local\\Temp\\tmplibrfded\n",
      "2022-06-01 14:23:55,897 File train has 741 questions and 897 answers.\n",
      "2022-06-01 14:23:56,356 https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/test.txt not found in cache, downloading to C:\\Users\\Asia\\AppData\\Local\\Temp\\tmp8pvii32g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1093520B [00:00, 7741086.95B/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:23:56,978 copying C:\\Users\\Asia\\AppData\\Local\\Temp\\tmp8pvii32g to cache at C:\\Users\\Asia\\.flair\\datasets\\ner_english_stackoverflow\\test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:23:57,142 removing temp file C:\\Users\\Asia\\AppData\\Local\\Temp\\tmp8pvii32g\n",
      "2022-06-01 14:23:57,350 File test has 249 questions and 315 answers.\n",
      "2022-06-01 14:23:57,816 https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/dev.txt not found in cache, downloading to C:\\Users\\Asia\\AppData\\Local\\Temp\\tmp7vfkw8xf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1003586B [00:00, 15139839.71B/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:23:58,005 copying C:\\Users\\Asia\\AppData\\Local\\Temp\\tmp7vfkw8xf to cache at C:\\Users\\Asia\\.flair\\datasets\\ner_english_stackoverflow\\dev.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:23:58,223 removing temp file C:\\Users\\Asia\\AppData\\Local\\Temp\\tmp7vfkw8xf\n",
      "2022-06-01 14:23:58,540 File dev has 247 questions and 289 answers.\n",
      "2022-06-01 14:23:58,540 Reading data from C:\\Users\\Asia\\.flair\\datasets\\ner_english_stackoverflow\n",
      "2022-06-01 14:23:58,540 Train: C:\\Users\\Asia\\.flair\\datasets\\ner_english_stackoverflow\\train.txt\n",
      "2022-06-01 14:23:58,540 Dev: C:\\Users\\Asia\\.flair\\datasets\\ner_english_stackoverflow\\dev.txt\n",
      "2022-06-01 14:23:58,540 Test: C:\\Users\\Asia\\.flair\\datasets\\ner_english_stackoverflow\\test.txt\n",
      "2022-06-01 14:24:10,085 Filtering empty sentences\n",
      "2022-06-01 14:24:10,158 Corpus: 926 train + 294 dev + 311 test sentences\n",
      "2022-06-01 14:24:10,175 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "926it [00:00, 19643.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:24:10,227 Dictionary created for label 'ner' with 20 values: Class (seen 147 times), Application (seen 119 times), Variable (seen 105 times), Code_Block (seen 92 times), User_Interface_Element (seen 91 times), Library (seen 77 times), Function (seen 67 times), Data_Structure (seen 62 times), Language (seen 59 times), Data_Type (seen 42 times), Version (seen 36 times), File_Name (seen 33 times), Operating_System (seen 27 times), File_Type (seen 23 times), HTML_XML_Tag (seen 21 times), Device (seen 20 times), Website (seen 13 times), User_Name (seen 9 times), Algorithm (seen 6 times)\n",
      "\n",
      "\n",
      "Etykiety do wykrycia\n",
      "Dictionary with 20 tags: <unk>, Class, Application, Variable, Code_Block, User_Interface_Element, Library, Function, Data_Structure, Language, Data_Type, Version, File_Name, Operating_System, File_Type, HTML_XML_Tag, Device, Website, User_Name, Algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flair.datasets import NER_ENGLISH_STACKOVERFLOW    # zbiór otagowanych postów na Stacku\n",
    "\n",
    "corpus = NER_ENGLISH_STACKOVERFLOW().downsample(0.1)   # pobieramy korpus i zmniejszamy jego wielkość\n",
    "corpus.filter_empty_sentences()                         # usuwamy puste zdania\n",
    "\n",
    "label_type = 'ner'   # identyfikator pod którym możemy dostać typy etykiet\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)    # TODO\n",
    "print('\\n\\nEtykiety do wykrycia')\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlz60uuR83oR"
   },
   "source": [
    "# Embeddingi\n",
    "\n",
    "W narzędziu Flair możemy bardzo prosto składać ze sobą różne embeddingi. \n",
    "\n",
    "**Zad2 (2 punkty):** Zapoznaj się z działaniem `StackedEmbeddings` opisanego w https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md i zbuduj embeddingi zawierające reprezentacje pochodzące zarówno z Glove jak i Flairowe, oparte na `news-forward`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yzCwq37iwFo4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"The\"\n",
      "tensor([-3.8194e-02, -2.4487e-01,  7.2812e-01,  ..., -4.4014e-04,\n",
      "        -3.9301e-02,  1.0601e-02])\n",
      "Token[1]: \"grass\"\n",
      "tensor([-8.1353e-01,  9.4042e-01, -2.4048e-01,  ..., -3.7749e-04,\n",
      "        -2.3563e-02,  1.1700e-02])\n",
      "Token[2]: \"is\"\n",
      "tensor([-0.5426,  0.4148,  1.0322,  ..., -0.0061,  0.0112,  0.0100])\n",
      "Token[3]: \"green\"\n",
      "tensor([-0.6791,  0.3491, -0.2398,  ..., -0.0026, -0.0118,  0.0455])\n",
      "Token[4]: \".\"\n",
      "tensor([-3.3979e-01,  2.0941e-01,  4.6348e-01,  ..., -2.3405e-04,\n",
      "         3.8688e-03,  5.7725e-03])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "\n",
    "\n",
    "embeddings = StackedEmbeddings([glove_embedding,flair_embedding_forward,]) \n",
    "\n",
    "sentence = Sentence('The grass is green .')\n",
    "embeddings.embed(sentence)\n",
    "\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Wxgw7Uc91e3"
   },
   "source": [
    "# Tagger i trainer\n",
    "\n",
    "**Zadanie 3 (2 punkty)** Bazując na treściach opisanych w https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md przygotuj obiekt `SequenceTagger`, którego rozmiar wartswy ukrytej wyniesie 256. Do obiektu tego przekażemy stworzone wcześniej embeddingi, słownik `label_dict` i nazwę kolumny z etykietą ze zmiennej `label_type`. Ustawmy `use_crf` na True.\n",
    "\n",
    "Przygotuj obiekt `ModelTrainer`, który przyjmie zarówno nasz korpus jak i stworzony przed chwilą `SequenceTagger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZqglzMPP92Fq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:33:42,631 SequenceTagger predicts: Dictionary with 77 tags: O, S-Class, B-Class, E-Class, I-Class, S-Application, B-Application, E-Application, I-Application, S-Variable, B-Variable, E-Variable, I-Variable, S-Code_Block, B-Code_Block, E-Code_Block, I-Code_Block, S-User_Interface_Element, B-User_Interface_Element, E-User_Interface_Element, I-User_Interface_Element, S-Library, B-Library, E-Library, I-Library, S-Function, B-Function, E-Function, I-Function, S-Data_Structure, B-Data_Structure, E-Data_Structure, I-Data_Structure, S-Language, B-Language, E-Language, I-Language, S-Data_Type, B-Data_Type, E-Data_Type, I-Data_Type, S-Version, B-Version, E-Version, I-Version, S-File_Name, B-File_Name, E-File_Name, I-File_Name, S-Operating_System\n",
      "2022-06-01 14:33:42,790 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:33:42,790 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'glove'\n",
      "      (embedding): Embedding(400001, 100)\n",
      "    )\n",
      "    (list_embedding_1): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.05, inplace=False)\n",
      "        (encoder): Embedding(300, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=2148, out_features=2148, bias=True)\n",
      "  (rnn): LSTM(2148, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=79, bias=True)\n",
      "  (loss_function): ViterbiLoss()\n",
      "  (crf): CRF()\n",
      ")\"\n",
      "2022-06-01 14:33:42,790 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:33:42,797 Corpus: \"Corpus: 926 train + 294 dev + 311 test sentences\"\n",
      "2022-06-01 14:33:42,798 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:33:42,798 Parameters:\n",
      "2022-06-01 14:33:42,798  - learning_rate: \"0.100000\"\n",
      "2022-06-01 14:33:42,806  - mini_batch_size: \"32\"\n",
      "2022-06-01 14:33:42,808  - patience: \"3\"\n",
      "2022-06-01 14:33:42,810  - anneal_factor: \"0.5\"\n",
      "2022-06-01 14:33:42,810  - max_epochs: \"5\"\n",
      "2022-06-01 14:33:42,810  - shuffle: \"True\"\n",
      "2022-06-01 14:33:42,810  - train_with_dev: \"False\"\n",
      "2022-06-01 14:33:42,810  - batch_growth_annealing: \"False\"\n",
      "2022-06-01 14:33:42,821 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:33:42,822 Model training base path: \"resources\\taggers\\example-upos\"\n",
      "2022-06-01 14:33:42,825 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:33:42,826 Device: cpu\n",
      "2022-06-01 14:33:42,829 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:33:42,829 Embeddings storage mode: cpu\n",
      "2022-06-01 14:33:42,829 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:34:02,224 epoch 1 - iter 2/29 - loss 3.41348560 - samples/sec: 3.30 - lr: 0.100000\n",
      "2022-06-01 14:34:14,404 epoch 1 - iter 4/29 - loss 2.33190893 - samples/sec: 5.25 - lr: 0.100000\n",
      "2022-06-01 14:34:26,450 epoch 1 - iter 6/29 - loss 1.87831627 - samples/sec: 5.32 - lr: 0.100000\n",
      "2022-06-01 14:34:41,951 epoch 1 - iter 8/29 - loss 1.57711710 - samples/sec: 4.13 - lr: 0.100000\n",
      "2022-06-01 14:34:53,793 epoch 1 - iter 10/29 - loss 1.44449984 - samples/sec: 5.41 - lr: 0.100000\n",
      "2022-06-01 14:35:06,728 epoch 1 - iter 12/29 - loss 1.28198710 - samples/sec: 4.95 - lr: 0.100000\n",
      "2022-06-01 14:35:19,108 epoch 1 - iter 14/29 - loss 1.22170634 - samples/sec: 5.17 - lr: 0.100000\n",
      "2022-06-01 14:35:36,394 epoch 1 - iter 16/29 - loss 1.16855373 - samples/sec: 3.70 - lr: 0.100000\n",
      "2022-06-01 14:35:57,253 epoch 1 - iter 18/29 - loss 1.13407199 - samples/sec: 3.07 - lr: 0.100000\n",
      "2022-06-01 14:36:14,453 epoch 1 - iter 20/29 - loss 1.09265785 - samples/sec: 3.73 - lr: 0.100000\n",
      "2022-06-01 14:36:29,620 epoch 1 - iter 22/29 - loss 1.04949930 - samples/sec: 4.22 - lr: 0.100000\n",
      "2022-06-01 14:36:44,223 epoch 1 - iter 24/29 - loss 1.01533020 - samples/sec: 4.38 - lr: 0.100000\n",
      "2022-06-01 14:37:03,191 epoch 1 - iter 26/29 - loss 0.98295296 - samples/sec: 3.37 - lr: 0.100000\n",
      "2022-06-01 14:37:21,472 epoch 1 - iter 28/29 - loss 0.96642997 - samples/sec: 3.50 - lr: 0.100000\n",
      "2022-06-01 14:37:27,495 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:37:27,500 EPOCH 1 done: loss 0.9546 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:55<00:00,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:38:23,166 Evaluating as a multi-label problem: False\n",
      "2022-06-01 14:38:23,223 DEV : loss 0.6544909477233887 - f1-score (micro avg)  0.0055\n",
      "2022-06-01 14:38:23,274 BAD EPOCHS (no improvement): 0\n",
      "2022-06-01 14:38:23,274 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:38:25,391 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:38:33,355 epoch 2 - iter 2/29 - loss 0.62154284 - samples/sec: 8.06 - lr: 0.100000\n",
      "2022-06-01 14:38:43,088 epoch 2 - iter 4/29 - loss 0.58837455 - samples/sec: 6.58 - lr: 0.100000\n",
      "2022-06-01 14:38:47,948 epoch 2 - iter 6/29 - loss 0.61737519 - samples/sec: 13.18 - lr: 0.100000\n",
      "2022-06-01 14:38:53,649 epoch 2 - iter 8/29 - loss 0.61750186 - samples/sec: 11.23 - lr: 0.100000\n",
      "2022-06-01 14:38:58,595 epoch 2 - iter 10/29 - loss 0.62349695 - samples/sec: 12.94 - lr: 0.100000\n",
      "2022-06-01 14:39:04,287 epoch 2 - iter 12/29 - loss 0.62599367 - samples/sec: 11.24 - lr: 0.100000\n",
      "2022-06-01 14:39:08,329 epoch 2 - iter 14/29 - loss 0.62428650 - samples/sec: 15.86 - lr: 0.100000\n",
      "2022-06-01 14:39:14,815 epoch 2 - iter 16/29 - loss 0.63648196 - samples/sec: 9.87 - lr: 0.100000\n",
      "2022-06-01 14:39:19,600 epoch 2 - iter 18/29 - loss 0.63311866 - samples/sec: 13.38 - lr: 0.100000\n",
      "2022-06-01 14:39:26,245 epoch 2 - iter 20/29 - loss 0.62652461 - samples/sec: 9.64 - lr: 0.100000\n",
      "2022-06-01 14:39:36,492 epoch 2 - iter 22/29 - loss 0.61208918 - samples/sec: 6.25 - lr: 0.100000\n",
      "2022-06-01 14:39:41,220 epoch 2 - iter 24/29 - loss 0.60566127 - samples/sec: 13.55 - lr: 0.100000\n",
      "2022-06-01 14:39:45,606 epoch 2 - iter 26/29 - loss 0.59640194 - samples/sec: 14.60 - lr: 0.100000\n",
      "2022-06-01 14:39:51,458 epoch 2 - iter 28/29 - loss 0.59814690 - samples/sec: 10.94 - lr: 0.100000\n",
      "2022-06-01 14:39:53,228 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:39:53,230 EPOCH 2 done: loss 0.5999 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:39:58,445 Evaluating as a multi-label problem: False\n",
      "2022-06-01 14:39:58,466 DEV : loss 0.5762297511100769 - f1-score (micro avg)  0.0053\n",
      "2022-06-01 14:39:58,515 BAD EPOCHS (no improvement): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:39:58,515 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:40:02,885 epoch 3 - iter 2/29 - loss 0.41762926 - samples/sec: 14.68 - lr: 0.100000\n",
      "2022-06-01 14:40:10,481 epoch 3 - iter 4/29 - loss 0.47227542 - samples/sec: 8.43 - lr: 0.100000\n",
      "2022-06-01 14:40:15,181 epoch 3 - iter 6/29 - loss 0.49683903 - samples/sec: 13.62 - lr: 0.100000\n",
      "2022-06-01 14:40:19,826 epoch 3 - iter 8/29 - loss 0.54611827 - samples/sec: 13.79 - lr: 0.100000\n",
      "2022-06-01 14:40:25,806 epoch 3 - iter 10/29 - loss 0.52690637 - samples/sec: 10.71 - lr: 0.100000\n",
      "2022-06-01 14:40:31,557 epoch 3 - iter 12/29 - loss 0.51981469 - samples/sec: 11.13 - lr: 0.100000\n",
      "2022-06-01 14:40:38,500 epoch 3 - iter 14/29 - loss 0.52326135 - samples/sec: 9.22 - lr: 0.100000\n",
      "2022-06-01 14:40:44,532 epoch 3 - iter 16/29 - loss 0.53905127 - samples/sec: 10.62 - lr: 0.100000\n",
      "2022-06-01 14:40:48,720 epoch 3 - iter 18/29 - loss 0.54055660 - samples/sec: 15.30 - lr: 0.100000\n",
      "2022-06-01 14:40:54,676 epoch 3 - iter 20/29 - loss 0.54212425 - samples/sec: 10.75 - lr: 0.100000\n",
      "2022-06-01 14:41:04,266 epoch 3 - iter 22/29 - loss 0.53792570 - samples/sec: 6.67 - lr: 0.100000\n",
      "2022-06-01 14:41:10,428 epoch 3 - iter 24/29 - loss 0.54561501 - samples/sec: 10.39 - lr: 0.100000\n",
      "2022-06-01 14:41:14,700 epoch 3 - iter 26/29 - loss 0.54752042 - samples/sec: 14.98 - lr: 0.100000\n",
      "2022-06-01 14:41:19,236 epoch 3 - iter 28/29 - loss 0.55295260 - samples/sec: 14.12 - lr: 0.100000\n",
      "2022-06-01 14:41:20,765 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:41:20,768 EPOCH 3 done: loss 0.5535 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:41:25,916 Evaluating as a multi-label problem: False\n",
      "2022-06-01 14:41:25,945 DEV : loss 0.5062013268470764 - f1-score (micro avg)  0.0374\n",
      "2022-06-01 14:41:25,992 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:41:25,995 saving best model\n",
      "2022-06-01 14:41:27,741 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:41:34,213 epoch 4 - iter 2/29 - loss 0.59642499 - samples/sec: 9.90 - lr: 0.100000\n",
      "2022-06-01 14:41:40,546 epoch 4 - iter 4/29 - loss 0.58600300 - samples/sec: 10.11 - lr: 0.100000\n",
      "2022-06-01 14:41:45,576 epoch 4 - iter 6/29 - loss 0.58386443 - samples/sec: 12.72 - lr: 0.100000\n",
      "2022-06-01 14:41:52,606 epoch 4 - iter 8/29 - loss 0.55467333 - samples/sec: 9.11 - lr: 0.100000\n",
      "2022-06-01 14:41:57,356 epoch 4 - iter 10/29 - loss 0.55272091 - samples/sec: 13.49 - lr: 0.100000\n",
      "2022-06-01 14:42:01,275 epoch 4 - iter 12/29 - loss 0.52707172 - samples/sec: 16.33 - lr: 0.100000\n",
      "2022-06-01 14:42:06,162 epoch 4 - iter 14/29 - loss 0.53597374 - samples/sec: 13.10 - lr: 0.100000\n",
      "2022-06-01 14:42:11,597 epoch 4 - iter 16/29 - loss 0.52270930 - samples/sec: 11.78 - lr: 0.100000\n",
      "2022-06-01 14:42:16,418 epoch 4 - iter 18/29 - loss 0.51933056 - samples/sec: 13.30 - lr: 0.100000\n",
      "2022-06-01 14:42:24,350 epoch 4 - iter 20/29 - loss 0.51559690 - samples/sec: 8.07 - lr: 0.100000\n",
      "2022-06-01 14:42:33,156 epoch 4 - iter 22/29 - loss 0.51893066 - samples/sec: 7.27 - lr: 0.100000\n",
      "2022-06-01 14:42:39,815 epoch 4 - iter 24/29 - loss 0.51872953 - samples/sec: 9.62 - lr: 0.100000\n",
      "2022-06-01 14:42:43,612 epoch 4 - iter 26/29 - loss 0.51539441 - samples/sec: 16.87 - lr: 0.100000\n",
      "2022-06-01 14:42:47,617 epoch 4 - iter 28/29 - loss 0.51743285 - samples/sec: 15.99 - lr: 0.100000\n",
      "2022-06-01 14:42:50,769 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:42:50,769 EPOCH 4 done: loss 0.5162 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:42:56,071 Evaluating as a multi-label problem: False\n",
      "2022-06-01 14:42:56,097 DEV : loss 0.47923287749290466 - f1-score (micro avg)  0.0238\n",
      "2022-06-01 14:42:56,145 BAD EPOCHS (no improvement): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:42:56,145 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:43:03,253 epoch 5 - iter 2/29 - loss 0.56557632 - samples/sec: 9.01 - lr: 0.100000\n",
      "2022-06-01 14:43:07,270 epoch 5 - iter 4/29 - loss 0.46193980 - samples/sec: 15.96 - lr: 0.100000\n",
      "2022-06-01 14:43:13,500 epoch 5 - iter 6/29 - loss 0.50142779 - samples/sec: 10.28 - lr: 0.100000\n",
      "2022-06-01 14:43:18,027 epoch 5 - iter 8/29 - loss 0.48265585 - samples/sec: 14.15 - lr: 0.100000\n",
      "2022-06-01 14:43:22,242 epoch 5 - iter 10/29 - loss 0.49221819 - samples/sec: 15.19 - lr: 0.100000\n",
      "2022-06-01 14:43:27,566 epoch 5 - iter 12/29 - loss 0.47098771 - samples/sec: 12.02 - lr: 0.100000\n",
      "2022-06-01 14:43:32,545 epoch 5 - iter 14/29 - loss 0.46919839 - samples/sec: 12.86 - lr: 0.100000\n",
      "2022-06-01 14:43:39,121 epoch 5 - iter 16/29 - loss 0.47434462 - samples/sec: 9.73 - lr: 0.100000\n",
      "2022-06-01 14:43:49,168 epoch 5 - iter 18/29 - loss 0.47563757 - samples/sec: 6.37 - lr: 0.100000\n",
      "2022-06-01 14:43:55,015 epoch 5 - iter 20/29 - loss 0.48430581 - samples/sec: 10.95 - lr: 0.100000\n",
      "2022-06-01 14:44:00,479 epoch 5 - iter 22/29 - loss 0.48546859 - samples/sec: 11.71 - lr: 0.100000\n",
      "2022-06-01 14:44:05,590 epoch 5 - iter 24/29 - loss 0.48323890 - samples/sec: 12.52 - lr: 0.100000\n",
      "2022-06-01 14:44:12,186 epoch 5 - iter 26/29 - loss 0.47738669 - samples/sec: 9.71 - lr: 0.100000\n",
      "2022-06-01 14:44:15,937 epoch 5 - iter 28/29 - loss 0.47450206 - samples/sec: 17.09 - lr: 0.100000\n",
      "2022-06-01 14:44:19,023 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:44:19,023 EPOCH 5 done: loss 0.4762 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:44:24,403 Evaluating as a multi-label problem: False\n",
      "2022-06-01 14:44:24,435 DEV : loss 0.45542776584625244 - f1-score (micro avg)  0.1339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:44:24,489 BAD EPOCHS (no improvement): 0\n",
      "2022-06-01 14:44:24,493 saving best model\n",
      "2022-06-01 14:44:28,435 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:44:28,456 loading file resources\\taggers\\example-upos\\best-model.pt\n",
      "2022-06-01 14:44:29,810 SequenceTagger predicts: Dictionary with 79 tags: O, S-Class, B-Class, E-Class, I-Class, S-Application, B-Application, E-Application, I-Application, S-Variable, B-Variable, E-Variable, I-Variable, S-Code_Block, B-Code_Block, E-Code_Block, I-Code_Block, S-User_Interface_Element, B-User_Interface_Element, E-User_Interface_Element, I-User_Interface_Element, S-Library, B-Library, E-Library, I-Library, S-Function, B-Function, E-Function, I-Function, S-Data_Structure, B-Data_Structure, E-Data_Structure, I-Data_Structure, S-Language, B-Language, E-Language, I-Language, S-Data_Type, B-Data_Type, E-Data_Type, I-Data_Type, S-Version, B-Version, E-Version, I-Version, S-File_Name, B-File_Name, E-File_Name, I-File_Name, S-Operating_System\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:58<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:45:28,818 Evaluating as a multi-label problem: False\n",
      "2022-06-01 14:45:28,850 0.156\t0.0503\t0.0761\t0.045\n",
      "2022-06-01 14:45:28,853 \n",
      "Results:\n",
      "- F-score (micro) 0.0761\n",
      "- F-score (macro) 0.025\n",
      "- Accuracy 0.045\n",
      "\n",
      "By class:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                 Class     0.2381    0.2128    0.2247        47\n",
      "           Application     0.3529    0.1364    0.1967        44\n",
      "              Variable     0.0000    0.0000    0.0000        32\n",
      "            Code_Block     0.0000    0.0000    0.0000        23\n",
      "              Language     0.0769    0.0417    0.0541        24\n",
      "              Function     0.0000    0.0000    0.0000        23\n",
      "User_Interface_Element     0.0000    0.0000    0.0000        26\n",
      "        Data_Structure     0.0000    0.0000    0.0000        25\n",
      "               Library     0.0000    0.0000    0.0000        16\n",
      "               Version     0.0000    0.0000    0.0000        14\n",
      "             File_Type     0.0000    0.0000    0.0000        14\n",
      "             File_Name     0.0000    0.0000    0.0000        12\n",
      "             Data_Type     0.0000    0.0000    0.0000        11\n",
      "      Operating_System     0.0000    0.0000    0.0000         9\n",
      "          HTML_XML_Tag     0.0000    0.0000    0.0000         6\n",
      "                Device     0.0000    0.0000    0.0000         6\n",
      "               Website     0.0000    0.0000    0.0000         3\n",
      "             User_Name     0.0000    0.0000    0.0000         2\n",
      "             Algorithm     0.0000    0.0000    0.0000         1\n",
      "\n",
      "             micro avg     0.1560    0.0503    0.0761       338\n",
      "             macro avg     0.0352    0.0206    0.0250       338\n",
      "          weighted avg     0.0845    0.0503    0.0607       338\n",
      "\n",
      "2022-06-01 14:45:28,856 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-01 14:45:28,888 loading file resources/taggers/example-upos/final-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:45:30,255 SequenceTagger predicts: Dictionary with 79 tags: O, S-Class, B-Class, E-Class, I-Class, S-Application, B-Application, E-Application, I-Application, S-Variable, B-Variable, E-Variable, I-Variable, S-Code_Block, B-Code_Block, E-Code_Block, I-Code_Block, S-User_Interface_Element, B-User_Interface_Element, E-User_Interface_Element, I-User_Interface_Element, S-Library, B-Library, E-Library, I-Library, S-Function, B-Function, E-Function, I-Function, S-Data_Structure, B-Data_Structure, E-Data_Structure, I-Data_Structure, S-Language, B-Language, E-Language, I-Language, S-Data_Type, B-Data_Type, E-Data_Type, I-Data_Type, S-Version, B-Version, E-Version, I-Version, S-File_Name, B-File_Name, E-File_Name, I-File_Name, S-Operating_System\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "\n",
    "tagger = SequenceTagger(hidden_size = 256, embeddings = embeddings, tag_dictionary=label_dict, tag_type=label_type, use_crf=True)\n",
    "trainer = ModelTrainer(tagger, corpus)   # TODO\n",
    "\n",
    "\n",
    "#stworzony trainer możemy zacząć trenować!\n",
    "trainer.train('resources/taggers/example-upos',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=5)\n",
    "\n",
    "# a kiedy wytrenujemy, wczytujemy najlepszy model.\n",
    "model = SequenceTagger.load('resources/taggers/example-upos/final-model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxrd6m0H_-SJ"
   },
   "source": [
    "# Predykcja z udziałem modelu\n",
    "\n",
    "Jeśli model został wytrenowany, poniżej znajdziemy fragment kodu, który może wykryć encje w zdaniach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Koq76zqawM3P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"huge files can be opened from Python 3 .\" → [\"Python\"/Language, \"3\"/Code_Block]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# Jeśli nasz model się wyuczył, powinien wykryć Python jako język.\n",
    "# Uwaga, ponieważ pracujemy na niewielkim podzbiorze zbioru danych (downsample(0.1) próbkuje 10%), \n",
    "# otrzymywane wyniki mogą być kiepskiej jakości, najlepiej zwiększyść ilość danych \n",
    "# jeśli pracujemy w domu.\n",
    "sentence = Sentence('huge files can be opened from Python 3.')   # stwórz obiekt zdania\n",
    "model.predict(sentence)                                         # wykryj encje nazwane\n",
    "print(sentence.to_tagged_string())                              # wyświetl zdanie i wykryte w nim encje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOgY6c2y2eWb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
